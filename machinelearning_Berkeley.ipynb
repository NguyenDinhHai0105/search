{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qrEjjv1DpCXa",
        "7txAucTGpxC-",
        "cK1nxf5OvCSs",
        "JA32mQtJvOfp",
        "xAJWsO663EaT"
      ],
      "gpuType": "T4",
      "mount_file_id": "10ktcfAmYcQS3ef7-bAOD4JNA0LWVc6t_",
      "authorship_tag": "ABX9TyMlOE87vTDx+j0Mb0KvqzBA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## nn"
      ],
      "metadata": {
        "id": "qrEjjv1DpCXa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_-H7Lp1mm5xt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def format_shape(shape):\n",
        "    return \"x\".join(map(str, shape)) if shape else \"()\"\n",
        "\n",
        "class Node(object):\n",
        "    def __repr__(self):\n",
        "        return \"<{} shape={} at {}>\".format(\n",
        "            type(self).__name__, format_shape(self.data.shape), hex(id(self)))\n",
        "\n",
        "class DataNode(Node):\n",
        "    \"\"\"\n",
        "    DataNode is the parent class for Parameter and Constant nodes.\n",
        "\n",
        "    You should not need to use this class directly.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.parents = []\n",
        "        self.data = data\n",
        "\n",
        "    def _forward(self, *inputs):\n",
        "        return self.data\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(gradient, *inputs):\n",
        "        return []\n",
        "\n",
        "class Parameter(DataNode):\n",
        "    \"\"\"\n",
        "    A Parameter node stores parameters used in a neural network (or perceptron).\n",
        "\n",
        "    Use the the `update` method to update parameters when training the\n",
        "    perceptron or neural network.\n",
        "    \"\"\"\n",
        "    def __init__(self, *shape):\n",
        "        assert len(shape) == 2, (\n",
        "            \"Shape must have 2 dimensions, instead has {}\".format(len(shape)))\n",
        "        assert all(isinstance(dim, int) and dim > 0 for dim in shape), (\n",
        "            \"Shape must consist of positive integers, got {!r}\".format(shape))\n",
        "        limit = np.sqrt(3.0 / np.mean(shape))\n",
        "        data = np.random.uniform(low=-limit, high=limit, size=shape)\n",
        "        super().__init__(data)\n",
        "\n",
        "    def update(self, direction, multiplier):\n",
        "        assert isinstance(direction, Constant), (\n",
        "            \"Update direction must be a {} node, instead has type {!r}\".format(\n",
        "                Constant.__name__, type(direction).__name__))\n",
        "        assert direction.data.shape == self.data.shape, (\n",
        "            \"Update direction shape {} does not match parameter shape \"\n",
        "            \"{}\".format(\n",
        "                format_shape(direction.data.shape),\n",
        "                format_shape(self.data.shape)))\n",
        "        assert isinstance(multiplier, (int, float)), (\n",
        "            \"Multiplier must be a Python scalar, instead has type {!r}\".format(\n",
        "                type(multiplier).__name__))\n",
        "        self.data += multiplier * direction.data\n",
        "        assert np.all(np.isfinite(self.data)), (\n",
        "            \"Parameter contains NaN or infinity after update, cannot continue\")\n",
        "\n",
        "class Constant(DataNode):\n",
        "    \"\"\"\n",
        "    A Constant node is used to represent:\n",
        "    * Input features\n",
        "    * Output labels\n",
        "    * Gradients computed by back-propagation\n",
        "\n",
        "    You should not need to construct any Constant nodes directly; they will\n",
        "    instead be provided by either the dataset or when you call `gradients`.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        assert isinstance(data, np.ndarray), (\n",
        "            \"Data should be a numpy array, instead has type {!r}\".format(\n",
        "                type(data).__name__))\n",
        "        assert np.issubdtype(data.dtype, np.floating), (\n",
        "            \"Data should be a float array, instead has data type {!r}\".format(\n",
        "                data.dtype))\n",
        "        super().__init__(data)\n",
        "\n",
        "class FunctionNode(Node):\n",
        "    \"\"\"\n",
        "    A FunctionNode represents a value that is computed based on other nodes.\n",
        "    The FunctionNode class performs necessary book-keeping to compute gradients.\n",
        "    \"\"\"\n",
        "    def __init__(self, *parents):\n",
        "        assert all(isinstance(parent, Node) for parent in parents), (\n",
        "            \"Inputs must be node objects, instead got types {!r}\".format(\n",
        "                tuple(type(parent).__name__ for parent in parents)))\n",
        "        self.parents = parents\n",
        "        self.data = self._forward(*(parent.data for parent in parents))\n",
        "\n",
        "class Add(FunctionNode):\n",
        "    \"\"\"\n",
        "    Adds matrices element-wise.\n",
        "\n",
        "    Usage: Add(x, y)\n",
        "    Inputs:\n",
        "        x: a Node with shape (batch_size x num_features)\n",
        "        y: a Node with the same shape as x\n",
        "    Output:\n",
        "        a Node with shape (batch_size x num_features)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _forward(*inputs):\n",
        "        assert len(inputs) == 2, \"Expected 2 inputs, got {}\".format(len(inputs))\n",
        "        assert inputs[0].ndim == 2, (\n",
        "            \"First input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[0].ndim))\n",
        "        assert inputs[1].ndim == 2, (\n",
        "            \"Second input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[1].ndim))\n",
        "        assert inputs[0].shape == inputs[1].shape, (\n",
        "            \"Input shapes should match, instead got {} and {}\".format(\n",
        "                format_shape(inputs[0].shape), format_shape(inputs[1].shape)))\n",
        "        return inputs[0] + inputs[1]\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(gradient, *inputs):\n",
        "        assert gradient.shape == inputs[0].shape\n",
        "        return [gradient, gradient]\n",
        "\n",
        "class AddBias(FunctionNode):\n",
        "    \"\"\"\n",
        "    Adds a bias vector to each feature vector\n",
        "\n",
        "    Usage: AddBias(features, bias)\n",
        "    Inputs:\n",
        "        features: a Node with shape (batch_size x num_features)\n",
        "        bias: a Node with shape (1 x num_features)\n",
        "    Output:\n",
        "        a Node with shape (batch_size x num_features)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _forward(*inputs):\n",
        "        assert len(inputs) == 2, \"Expected 2 inputs, got {}\".format(len(inputs))\n",
        "        assert inputs[0].ndim == 2, (\n",
        "            \"First input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[0].ndim))\n",
        "        assert inputs[1].ndim == 2, (\n",
        "            \"Second input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[1].ndim))\n",
        "        assert inputs[1].shape[0] == 1, (\n",
        "            \"First dimension of second input should be 1, instead got shape \"\n",
        "            \"{}\".format(format_shape(inputs[1].shape)))\n",
        "        assert inputs[0].shape[1] == inputs[1].shape[1], (\n",
        "            \"Second dimension of inputs should match, instead got shapes {} \"\n",
        "            \"and {}\".format(\n",
        "                format_shape(inputs[0].shape), format_shape(inputs[1].shape)))\n",
        "        return inputs[0] + inputs[1]\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(gradient, *inputs):\n",
        "        assert gradient.shape == inputs[0].shape\n",
        "        return [gradient, np.sum(gradient, axis=0, keepdims=True)]\n",
        "\n",
        "class DotProduct(FunctionNode):\n",
        "    \"\"\"\n",
        "    Batched dot product\n",
        "\n",
        "    Usage: DotProduct(features, weights)\n",
        "    Inputs:\n",
        "        features: a Node with shape (batch_size x num_features)\n",
        "        weights: a Node with shape (1 x num_features)\n",
        "    Output: a Node with shape (batch_size x 1)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _forward(*inputs):\n",
        "        assert len(inputs) == 2, \"Expected 2 inputs, got {}\".format(len(inputs))\n",
        "        assert inputs[0].ndim == 2, (\n",
        "            \"First input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[0].ndim))\n",
        "        assert inputs[1].ndim == 2, (\n",
        "            \"Second input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[1].ndim))\n",
        "        assert inputs[1].shape[0] == 1, (\n",
        "            \"First dimension of second input should be 1, instead got shape \"\n",
        "            \"{}\".format(format_shape(inputs[1].shape)))\n",
        "        assert inputs[0].shape[1] == inputs[1].shape[1], (\n",
        "            \"Second dimension of inputs should match, instead got shapes {} \"\n",
        "            \"and {}\".format(\n",
        "                format_shape(inputs[0].shape), format_shape(inputs[1].shape)))\n",
        "        return np.dot(inputs[0], inputs[1].T)\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(gradient, *inputs):\n",
        "        # assert gradient.shape[0] == inputs[0].shape[0]\n",
        "        # assert gradient.shape[1] == 1\n",
        "        # return [np.dot(gradient, inputs[1]), np.dot(gradient.T, inputs[0])]\n",
        "        raise NotImplementedError(\n",
        "            \"Backpropagation through DotProduct nodes is not needed in this \"\n",
        "            \"assignment\")\n",
        "\n",
        "class Linear(FunctionNode):\n",
        "    \"\"\"\n",
        "    Applies a linear transformation (matrix multiplication) to the input\n",
        "\n",
        "    Usage: Linear(features, weights)\n",
        "    Inputs:\n",
        "        features: a Node with shape (batch_size x input_features)\n",
        "        weights: a Node with shape (input_features x output_features)\n",
        "    Output: a node with shape (batch_size x output_features)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _forward(*inputs):\n",
        "        assert len(inputs) == 2, \"Expected 2 inputs, got {}\".format(len(inputs))\n",
        "        assert inputs[0].ndim == 2, (\n",
        "            \"First input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[0].ndim))\n",
        "        assert inputs[1].ndim == 2, (\n",
        "            \"Second input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[1].ndim))\n",
        "        assert inputs[0].shape[1] == inputs[1].shape[0], (\n",
        "            \"Second dimension of first input should match first dimension of \"\n",
        "            \"second input, instead got shapes {} and {}\".format(\n",
        "                format_shape(inputs[0].shape), format_shape(inputs[1].shape)))\n",
        "        return np.dot(inputs[0], inputs[1])\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(gradient, *inputs):\n",
        "        assert gradient.shape[0] == inputs[0].shape[0]\n",
        "        assert gradient.shape[1] == inputs[1].shape[1]\n",
        "        return [np.dot(gradient, inputs[1].T), np.dot(inputs[0].T, gradient)]\n",
        "\n",
        "class ReLU(FunctionNode):\n",
        "    \"\"\"\n",
        "    An element-wise Rectified Linear Unit nonlinearity: max(x, 0).\n",
        "    This nonlinearity replaces all negative entries in its input with zeros.\n",
        "\n",
        "    Usage: ReLU(x)\n",
        "    Input:\n",
        "        x: a Node with shape (batch_size x num_features)\n",
        "    Output: a Node with the same shape as x, but no negative entries\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _forward(*inputs):\n",
        "        assert len(inputs) == 1, \"Expected 1 input, got {}\".format(len(inputs))\n",
        "        assert inputs[0].ndim == 2, (\n",
        "            \"Input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[0].ndim))\n",
        "        return np.maximum(inputs[0], 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(gradient, *inputs):\n",
        "        assert gradient.shape == inputs[0].shape\n",
        "        return [gradient * np.where(inputs[0] > 0, 1.0, 0.0)]\n",
        "\n",
        "class SquareLoss(FunctionNode):\n",
        "    \"\"\"\n",
        "    This node first computes 0.5 * (a[i,j] - b[i,j])**2 at all positions (i,j)\n",
        "    in the inputs, which creates a (batch_size x dim) matrix. It then calculates\n",
        "    and returns the mean of all elements in this matrix.\n",
        "\n",
        "    Usage: SquareLoss(a, b)\n",
        "    Inputs:\n",
        "        a: a Node with shape (batch_size x dim)\n",
        "        b: a Node with shape (batch_size x dim)\n",
        "    Output: a scalar Node (containing a single floating-point number)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _forward(*inputs):\n",
        "        assert len(inputs) == 2, \"Expected 2 inputs, got {}\".format(len(inputs))\n",
        "        assert inputs[0].ndim == 2, (\n",
        "            \"First input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[0].ndim))\n",
        "        assert inputs[1].ndim == 2, (\n",
        "            \"Second input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[1].ndim))\n",
        "        assert inputs[0].shape == inputs[1].shape, (\n",
        "            \"Input shapes should match, instead got {} and {}\".format(\n",
        "                format_shape(inputs[0].shape), format_shape(inputs[1].shape)))\n",
        "        return np.mean(np.square(inputs[0] - inputs[1]) / 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(gradient, *inputs):\n",
        "        assert np.asarray(gradient).ndim == 0\n",
        "        return [\n",
        "            gradient * (inputs[0] - inputs[1]) / inputs[0].size,\n",
        "            gradient * (inputs[1] - inputs[0]) / inputs[0].size\n",
        "        ]\n",
        "\n",
        "class SoftmaxLoss(FunctionNode):\n",
        "    \"\"\"\n",
        "    A batched softmax loss, used for classification problems.\n",
        "\n",
        "    IMPORTANT: do not swap the order of the inputs to this node!\n",
        "\n",
        "    Usage: SoftmaxLoss(logits, labels)\n",
        "    Inputs:\n",
        "        logits: a Node with shape (batch_size x num_classes). Each row\n",
        "            represents the scores associated with that example belonging to a\n",
        "            particular class. A score can be an arbitrary real number.\n",
        "        labels: a Node with shape (batch_size x num_classes) that encodes the\n",
        "            correct labels for the examples. All entries must be non-negative\n",
        "            and the sum of values along each row should be 1.\n",
        "    Output: a scalar Node (containing a single floating-point number)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def log_softmax(logits):\n",
        "        log_probs = logits - np.max(logits, axis=1, keepdims=True)\n",
        "        log_probs -= np.log(np.sum(np.exp(log_probs), axis=1, keepdims=True))\n",
        "        return log_probs\n",
        "\n",
        "    @staticmethod\n",
        "    def _forward(*inputs):\n",
        "        assert len(inputs) == 2, \"Expected 2 inputs, got {}\".format(len(inputs))\n",
        "        assert inputs[0].ndim == 2, (\n",
        "            \"First input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[0].ndim))\n",
        "        assert inputs[1].ndim == 2, (\n",
        "            \"Second input should have 2 dimensions, instead has {}\".format(\n",
        "                inputs[1].ndim))\n",
        "        assert inputs[0].shape == inputs[1].shape, (\n",
        "            \"Input shapes should match, instead got {} and {}\".format(\n",
        "                format_shape(inputs[0].shape), format_shape(inputs[1].shape)))\n",
        "        assert np.all(inputs[1] >= 0), (\n",
        "            \"All entries in the labels input must be non-negative\")\n",
        "        assert np.allclose(np.sum(inputs[1], axis=1), 1), (\n",
        "            \"Labels input must sum to 1 along each row\")\n",
        "        log_probs = SoftmaxLoss.log_softmax(inputs[0])\n",
        "        return np.mean(-np.sum(inputs[1] * log_probs, axis=1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(gradient, *inputs):\n",
        "        assert np.asarray(gradient).ndim == 0\n",
        "        log_probs = SoftmaxLoss.log_softmax(inputs[0])\n",
        "        return [\n",
        "            gradient * (np.exp(log_probs) - inputs[1]) / inputs[0].shape[0],\n",
        "            gradient * -log_probs / inputs[0].shape[0]\n",
        "        ]\n",
        "\n",
        "def gradients(loss, parameters):\n",
        "    \"\"\"\n",
        "    Computes and returns the gradient of the loss with respect to the provided\n",
        "    parameters.\n",
        "\n",
        "    Usage: gradients(loss, parameters)\n",
        "    Inputs:\n",
        "        loss: a SquareLoss or SoftmaxLoss node\n",
        "        parameters: a list (or iterable) containing Parameter nodes\n",
        "    Output: a list of Constant objects, representing the gradient of the loss\n",
        "        with respect to each provided parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(loss, (SquareLoss, SoftmaxLoss)), (\n",
        "        \"Loss must be a loss node, instead has type {!r}\".format(\n",
        "            type(loss).__name__))\n",
        "    assert all(isinstance(parameter, Parameter) for parameter in parameters), (\n",
        "        \"Parameters must all have type {}, instead got types {!r}\".format(\n",
        "            Parameter.__name__,\n",
        "            tuple(type(parameter).__name__ for parameter in parameters)))\n",
        "    assert not hasattr(loss, \"used\"), (\n",
        "        \"Loss node has already been used for backpropagation, cannot reuse\")\n",
        "\n",
        "    loss.used = True\n",
        "\n",
        "    nodes = set()\n",
        "    tape = []\n",
        "\n",
        "    def visit(node):\n",
        "        if node not in nodes:\n",
        "            for parent in node.parents:\n",
        "                visit(parent)\n",
        "            nodes.add(node)\n",
        "            tape.append(node)\n",
        "\n",
        "    visit(loss)\n",
        "    nodes |= set(parameters)\n",
        "\n",
        "    grads = {node: np.zeros_like(node.data) for node in nodes}\n",
        "    grads[loss] = 1.0\n",
        "\n",
        "    for node in reversed(tape):\n",
        "        parent_grads = node._backward(\n",
        "            grads[node], *(parent.data for parent in node.parents))\n",
        "        for parent, parent_grad in zip(node.parents, parent_grads):\n",
        "            grads[parent] += parent_grad\n",
        "\n",
        "    return [Constant(grads[parameter]) for parameter in parameters]\n",
        "\n",
        "def as_scalar(node):\n",
        "    \"\"\"\n",
        "    Returns the value of a Node as a standard Python number. This only works\n",
        "    for nodes with one element (e.g. SquareLoss and SoftmaxLoss, as well as\n",
        "    DotProduct with a batch size of 1 element).\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(node, Node), (\n",
        "        \"Input must be a node object, instead has type {!r}\".format(\n",
        "            type(node).__name__))\n",
        "    assert node.data.size == 1, (\n",
        "        \"Node has shape {}, cannot convert to a scalar\".format(\n",
        "            format_shape(node.data.shape)))\n",
        "    # print(node.data[0])\n",
        "    return node.data[0][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## models"
      ],
      "metadata": {
        "id": "7txAucTGpxC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptronModel(object):\n",
        "    def __init__(self, dimensions):\n",
        "        \"\"\"\n",
        "        Initialize a new Perceptron instance.\n",
        "\n",
        "        A perceptron classifies data points as either belonging to a particular\n",
        "        class (+1) or not (-1). `dimensions` is the dimensionality of the data.\n",
        "        For example, dimensions=2 would mean that the perceptron must classify\n",
        "        2D points.\n",
        "        \"\"\"\n",
        "        self.w = Parameter(1, dimensions)\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"\n",
        "        Return a Parameter instance with the current weights of the perceptron.\n",
        "        \"\"\"\n",
        "        return self.w\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"\n",
        "        Calculates the score assigned by the perceptron to a data point x.\n",
        "\n",
        "        Inputs:\n",
        "            x: a node with shape (1 x dimensions)\n",
        "        Returns: a node containing a single number (the score)\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        return DotProduct(x, self.get_weights())\n",
        "\n",
        "    def get_prediction(self, x):\n",
        "        \"\"\"\n",
        "        Calculates the predicted class for a single data point `x`.\n",
        "\n",
        "        Returns: 1 or -1\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        return 1 if as_scalar(self.run(x)) >= 0 else -1\n",
        "\n",
        "    def train(self, dataset: Dataset):\n",
        "        \"\"\"\n",
        "        Train the perceptron until convergence.\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        \"Use Stachostic GD, so evaluation and param modification will based on each data point.\"\n",
        "        isAllCorrect = False\n",
        "\n",
        "        while not isAllCorrect:\n",
        "            isAllCorrect = True\n",
        "\n",
        "            for x, y in dataset.iterate_once(1):\n",
        "                y = as_scalar(y)\n",
        "                if self.get_prediction(x) != y:\n",
        "                    self.get_weights().update(x, y) # w -= x*y\n",
        "                    isAllCorrect = False\n",
        "\n",
        "\n",
        "class RegressionModel(object):\n",
        "    \"\"\"\n",
        "    A neural network model for approximating a function that maps from real\n",
        "    numbers to real numbers. The network should be sufficiently large to be able\n",
        "    to approximate sin(x) on the interval [-2pi, 2pi] to reasonable precision.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Initialize your model parameters here\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        hidden_layer_size = 88 # equivalent to number of output features\n",
        "        # 1 hidden layer (has weights denoted by w1 matrix)\n",
        "        self.w1 = Parameter(1, hidden_layer_size)\n",
        "        self.b1 = Parameter(1, hidden_layer_size)\n",
        "        # 1 node in output layer (has weights denoted by w2 matrix)\n",
        "        self.w2 = Parameter(hidden_layer_size, 1)\n",
        "        self.b2 = Parameter(1, 1)\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"\n",
        "        Runs the model for a batch of examples.\n",
        "\n",
        "        Inputs:\n",
        "            x: a node with shape (batch_size x 1)\n",
        "        Returns:\n",
        "            A node with shape (batch_size x 1) containing predicted y-values\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        linear_multi = Linear(x, self.w1)\n",
        "        z1 = AddBias(linear_multi, self.b1)\n",
        "        a1 = ReLU(z1)\n",
        "\n",
        "        linear_multi = Linear(a1, self.w2)\n",
        "        z2 = AddBias(linear_multi, self.b2)\n",
        "\n",
        "        return z2\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        \"\"\"\n",
        "        Computes the loss for a batch of examples.\n",
        "\n",
        "        Inputs:\n",
        "            x: a node with shape (batch_size x 1)\n",
        "            y: a node with shape (batch_size x 1), containing the true y-values\n",
        "                to be used for training\n",
        "        Returns: a loss node\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        predicted_y = self.run(x)\n",
        "        return SquareLoss(predicted_y, y)\n",
        "\n",
        "\n",
        "    def train(self, dataset: Dataset):\n",
        "        \"\"\"\n",
        "        Trains the model.\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        batch_size = dataset.x.shape[0]\n",
        "        learning_rate = 0.03\n",
        "\n",
        "        batch_number = 0\n",
        "        epoch = 0\n",
        "\n",
        "        def isAcceptable(loss_obj):\n",
        "            loss_function_value = loss_obj.data\n",
        "            # Final loss must be no more than 0.02 (set by autograder)\n",
        "            return True if loss_function_value <= 0.02 else False\n",
        "\n",
        "\n",
        "        # evaluate by loss function and modify params by GD on total dataset\n",
        "        for x, y in dataset.iterate_forever(batch_size):\n",
        "            batch_number = batch_number % 100 + 1\n",
        "            if batch_number == 1:\n",
        "                epoch += 1\n",
        "                print(\"Epoch: %s\" % epoch)\n",
        "\n",
        "            params = [self.w1, self.w2, self.b1, self.b2]\n",
        "            gradients = gradients(self.get_loss(x, y), params)\n",
        "\n",
        "            for i in range(len(gradients)):\n",
        "                params[i].update(gradients[i], -(learning_rate))\n",
        "\n",
        "\n",
        "            if isAcceptable(self.get_loss(x, y)):\n",
        "                return\n",
        "\n",
        "\n",
        "class DigitClassificationModel(object):\n",
        "    \"\"\"\n",
        "    A model for handwritten digit classification using the MNIST dataset.\n",
        "\n",
        "    Each handwritten digit is a 28x28 pixel grayscale image, which is flattened\n",
        "    into a 784-dimensional vector for the purposes of this model. Each entry in\n",
        "    the vector is a floating point number between 0 and 1.\n",
        "\n",
        "    The goal is to sort each digit into one of 10 classes (number 0 through 9).\n",
        "\n",
        "    (See RegressionModel for more information about the APIs of different\n",
        "    methods here. We recommend that you implement the RegressionModel before\n",
        "    working on this part of the project.)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Initialize your model parameters here\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        self.w1 = Parameter(784, 256)\n",
        "        self.b1 = Parameter(1, 256)\n",
        "        self.w2 = Parameter(256, 128)\n",
        "        self.b2 = Parameter(1, 128)\n",
        "        self.w3 = Parameter(128, 10)\n",
        "        self.b3 = Parameter(1, 10)\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"\n",
        "        Runs the model for a batch of examples.\n",
        "\n",
        "        Your model should predict a node with shape (batch_size x 10),\n",
        "        containing scores. Higher scores correspond to greater probability of\n",
        "        the image belonging to a particular class.\n",
        "\n",
        "        Inputs:\n",
        "            x: a node with shape (batch_size x 784)\n",
        "        Output:\n",
        "            A node with shape (batch_size x 10) containing predicted scores\n",
        "                (also called logits)\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        linear_multi = Linear(x, self.w1)\n",
        "        z1 = AddBias(linear_multi, self.b1)\n",
        "        a1 = ReLU(z1)\n",
        "\n",
        "        linear_multi = Linear(a1, self.w2)\n",
        "        z2 = AddBias(linear_multi, self.b2)\n",
        "        a2 = ReLU(z2)\n",
        "\n",
        "        linear_multi = Linear(a2, self.w3)\n",
        "        z3 = AddBias(linear_multi, self.b3)\n",
        "\n",
        "        return z3\n",
        "\n",
        "    def get_loss(self, x, y):\n",
        "        \"\"\"\n",
        "        Computes the loss for a batch of examples.\n",
        "\n",
        "        The correct labels `y` are represented as a node with shape\n",
        "        (batch_size x 10). Each row is a one-hot vector encoding the correct\n",
        "        digit class (0-9).\n",
        "\n",
        "        Inputs:\n",
        "            x: a node with shape (batch_size x 784)\n",
        "            y: a node with shape (batch_size x 10)\n",
        "        Returns: a loss node\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        prediction = self.run(x)\n",
        "        return SoftmaxLoss(prediction, y)\n",
        "\n",
        "\n",
        "    def train(self, dataset: Dataset):\n",
        "        \"\"\"\n",
        "        Trains the model.\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        learning_rate = 0.2\n",
        "        batch_size = int(dataset.x.shape[0] / 100)\n",
        "\n",
        "        batch_number = 0\n",
        "        epoch = 0\n",
        "\n",
        "        def isAcceptable(loss):\n",
        "            loss_value = loss.data\n",
        "            return True if loss_value < 0.015 else False\n",
        "\n",
        "\n",
        "        for x, y in dataset.iterate_forever(batch_size):\n",
        "            batch_number = batch_number % 100 + 1\n",
        "            if batch_number == 1:\n",
        "                epoch += 1\n",
        "                print(\"Epoch: %s\" % epoch)\n",
        "\n",
        "            params_list = [self.w1, self.b1, self.w2, self.b2, self.w3, self.b3]\n",
        "            local_v_gradients = gradients(self.get_loss(x, y), params_list)\n",
        "\n",
        "            for i in range(len(params_list)):\n",
        "                params_list[i].update(local_v_gradients[i], -(learning_rate)) #param -= gradient * learning_rate\n",
        "\n",
        "            if isAcceptable(self.get_loss(x, y)):\n",
        "                return\n",
        "\n",
        "class LanguageIDModel(object):\n",
        "    \"\"\"\n",
        "    A model for language identification at a single-word granularity.\n",
        "\n",
        "    (See RegressionModel for more information about the APIs of different\n",
        "    methods here. We recommend that you implement the RegressionModel before\n",
        "    working on this part of the project.)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Our dataset contains words from five different languages, and the\n",
        "        # combined alphabets of the five languages contain a total of 47 unique\n",
        "        # characters.\n",
        "        # You can refer to self.num_chars or len(self.languages) in your code\n",
        "        self.num_chars = 47\n",
        "        self.languages = [\"English\", \"Spanish\", \"Finnish\", \"Dutch\", \"Polish\"]\n",
        "\n",
        "        # Initialize your model parameters here\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "\n",
        "    def run(self, xs):\n",
        "        \"\"\"\n",
        "        Runs the model for a batch of examples.\n",
        "\n",
        "        Although words have different lengths, our data processing guarantees\n",
        "        that within a single batch, all words will be of the same length (L).\n",
        "\n",
        "        Here `xs` will be a list of length L. Each element of `xs` will be a\n",
        "        node with shape (batch_size x self.num_chars), where every row in the\n",
        "        array is a one-hot vector encoding of a character. For example, if we\n",
        "        have a batch of 8 three-letter words where the last word is \"cat\", then\n",
        "        xs[1] will be a node that contains a 1 at position (7, 0). Here the\n",
        "        index 7 reflects the fact that \"cat\" is the last word in the batch, and\n",
        "        the index 0 reflects the fact that the letter \"a\" is the inital (0th)\n",
        "        letter of our combined alphabet for this task.\n",
        "\n",
        "        Your model should use a Recurrent Neural Network to summarize the list\n",
        "        `xs` into a single node of shape (batch_size x hidden_size), for your\n",
        "        choice of hidden_size. It should then calculate a node of shape\n",
        "        (batch_size x 5) containing scores, where higher scores correspond to\n",
        "        greater probability of the word originating from a particular language.\n",
        "\n",
        "        Inputs:\n",
        "            xs: a list with L elements (one per character), where each element\n",
        "                is a node with shape (batch_size x self.num_chars)\n",
        "        Returns:\n",
        "            A node with shape (batch_size x 5) containing predicted scores\n",
        "                (also called logits)\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "\n",
        "    def get_loss(self, xs, y):\n",
        "        \"\"\"\n",
        "        Computes the loss for a batch of examples.\n",
        "\n",
        "        The correct labels `y` are represented as a node with shape\n",
        "        (batch_size x 5). Each row is a one-hot vector encoding the correct\n",
        "        language.\n",
        "\n",
        "        Inputs:\n",
        "            xs: a list with L elements (one per character), where each element\n",
        "                is a node with shape (batch_size x self.num_chars)\n",
        "            y: a node with shape (batch_size x 5)\n",
        "        Returns: a loss node\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Trains the model.\n",
        "        \"\"\"\n",
        "        \"*** YOUR CODE HERE ***\"\n"
      ],
      "metadata": {
        "id": "a6qrU45Dp3HB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## backend"
      ],
      "metadata": {
        "id": "XO8aLfEjo-T6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### class Dataset (must be run before models)"
      ],
      "metadata": {
        "id": "cK1nxf5OvCSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, x, y):\n",
        "        assert isinstance(x, np.ndarray)\n",
        "        assert isinstance(y, np.ndarray)\n",
        "        assert np.issubdtype(x.dtype, np.floating)\n",
        "        assert np.issubdtype(y.dtype, np.floating)\n",
        "        assert x.ndim == 2\n",
        "        assert y.ndim == 2\n",
        "        assert x.shape[0] == y.shape[0]\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def iterate_once(self, batch_size):\n",
        "        assert isinstance(batch_size, int) and batch_size > 0, (\n",
        "            \"Batch size should be a positive integer, got {!r}\".format(\n",
        "                batch_size))\n",
        "        assert self.x.shape[0] % batch_size == 0, (\n",
        "            \"Dataset size {:d} is not divisible by batch size {:d}\".format(\n",
        "                self.x.shape[0], batch_size))\n",
        "        index = 0\n",
        "        while index < self.x.shape[0]:\n",
        "            x = self.x[index:index + batch_size]\n",
        "            y = self.y[index:index + batch_size]\n",
        "            yield Constant(x), Constant(y)\n",
        "            index += batch_size\n",
        "\n",
        "    def iterate_forever(self, batch_size):\n",
        "        while True:\n",
        "            yield from self.iterate_once(batch_size)\n",
        "\n",
        "    def get_validation_accuracy(self):\n",
        "        raise NotImplementedError(\n",
        "            \"No validation data is available for this dataset. \"\n",
        "            \"In this assignment, only the Digit Classification and Language \"\n",
        "            \"Identification datasets have validation data.\")\n"
      ],
      "metadata": {
        "id": "K2EgRA3qvHNf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing & utility functions"
      ],
      "metadata": {
        "id": "JA32mQtJvOfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import time\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# import nn"
      ],
      "metadata": {
        "id": "N9CTJPQF21Lh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_graphics = True\n",
        "\n",
        "def maybe_sleep_and_close(seconds):\n",
        "    if use_graphics and plt.get_fignums():\n",
        "        time.sleep(seconds)\n",
        "        for fignum in plt.get_fignums():\n",
        "            fig = plt.figure(fignum)\n",
        "            plt.close(fig)\n",
        "            try:\n",
        "                # This raises a TclError on some Windows machines\n",
        "                fig.canvas.start_event_loop(1e-3)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "def get_data_path(filename):\n",
        "    # path = os.path.join(\n",
        "    #     os.path.dirname(__file__), os.pardir, \"data\", filename)\n",
        "    path = os.path.join(\n",
        "            \"/content/drive/Othercomputers\", \"My Laptop\", \"machinelearning/data\", filename)\n",
        "\n",
        "    # if not os.path.exists(path):\n",
        "    #     path = os.path.join(\n",
        "    #         os.path.dirname(__file__), \"data\", filename)\n",
        "    # if not os.path.exists(path):\n",
        "    #     path = os.path.join(\n",
        "    #         os.path.dirname(__file__), filename)\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        raise Exception(\"Could not find data file: {}\".format(filename))\n",
        "    return path"
      ],
      "metadata": {
        "id": "xVdH1X_k2vuP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset inheritors"
      ],
      "metadata": {
        "id": "xAJWsO663EaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Dataset(object):\n",
        "#     def __init__(self, x, y):\n",
        "#         assert isinstance(x, np.ndarray)\n",
        "#         assert isinstance(y, np.ndarray)\n",
        "#         assert np.issubdtype(x.dtype, np.floating)\n",
        "#         assert np.issubdtype(y.dtype, np.floating)\n",
        "#         assert x.ndim == 2\n",
        "#         assert y.ndim == 2\n",
        "#         assert x.shape[0] == y.shape[0]\n",
        "#         self.x = x\n",
        "#         self.y = y\n",
        "\n",
        "#     def iterate_once(self, batch_size):\n",
        "#         assert isinstance(batch_size, int) and batch_size > 0, (\n",
        "#             \"Batch size should be a positive integer, got {!r}\".format(\n",
        "#                 batch_size))\n",
        "#         assert self.x.shape[0] % batch_size == 0, (\n",
        "#             \"Dataset size {:d} is not divisible by batch size {:d}\".format(\n",
        "#                 self.x.shape[0], batch_size))\n",
        "#         index = 0\n",
        "#         while index < self.x.shape[0]:\n",
        "#             x = self.x[index:index + batch_size]\n",
        "#             y = self.y[index:index + batch_size]\n",
        "#             yield Constant(x), Constant(y)\n",
        "#             index += batch_size\n",
        "\n",
        "#     def iterate_forever(self, batch_size):\n",
        "#         while True:\n",
        "#             yield from self.iterate_once(batch_size)\n",
        "\n",
        "#     def get_validation_accuracy(self):\n",
        "#         raise NotImplementedError(\n",
        "#             \"No validation data is available for this dataset. \"\n",
        "#             \"In this assignment, only the Digit Classification and Language \"\n",
        "#             \"Identification datasets have validation data.\")\n",
        "\n",
        "class PerceptronDataset(Dataset):\n",
        "    def __init__(self, model):\n",
        "        points = 500\n",
        "        x = np.hstack([np.random.randn(points, 2), np.ones((points, 1))])\n",
        "        y = np.where(x[:, 0] + 2 * x[:, 1] - 1 >= 0, 1.0, -1.0)\n",
        "        super().__init__(x, np.expand_dims(y, axis=1))\n",
        "\n",
        "        self.model = model\n",
        "        self.epoch = 0\n",
        "\n",
        "        if use_graphics:\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            limits = np.array([-3.0, 3.0])\n",
        "            ax.set_xlim(limits)\n",
        "            ax.set_ylim(limits)\n",
        "            positive = ax.scatter(*x[y == 1, :-1].T, color=\"red\", marker=\"+\")\n",
        "            negative = ax.scatter(*x[y == -1, :-1].T, color=\"blue\", marker=\"_\")\n",
        "            line, = ax.plot([], [], color=\"black\")\n",
        "            text = ax.text(0.03, 0.97, \"\", transform=ax.transAxes, va=\"top\")\n",
        "            ax.legend([positive, negative], [1, -1])\n",
        "            plt.show(block=False)\n",
        "\n",
        "            self.fig = fig\n",
        "            self.limits = limits\n",
        "            self.line = line\n",
        "            self.text = text\n",
        "            self.last_update = time.time()\n",
        "\n",
        "    def iterate_once(self, batch_size):\n",
        "        self.epoch += 1\n",
        "\n",
        "        for i, (x, y) in enumerate(super().iterate_once(batch_size)):\n",
        "            yield x, y\n",
        "\n",
        "            if use_graphics and time.time() - self.last_update > 0.01:\n",
        "                w = self.model.get_weights().data.flatten()\n",
        "                limits = self.limits\n",
        "                if w[1] != 0:\n",
        "                    self.line.set_data(limits, (-w[0] * limits - w[2]) / w[1])\n",
        "                elif w[0] != 0:\n",
        "                    self.line.set_data(np.full(2, -w[2] / w[0]), limits)\n",
        "                else:\n",
        "                    self.line.set_data([], [])\n",
        "                self.text.set_text(\n",
        "                    \"epoch: {:,}\\npoint: {:,}/{:,}\\nweights: {}\".format(\n",
        "                        self.epoch, i * batch_size + 1, len(self.x), w))\n",
        "                self.fig.canvas.draw_idle()\n",
        "                self.fig.canvas.start_event_loop(1e-3)\n",
        "                self.last_update = time.time()\n",
        "\n",
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self, model):\n",
        "        x = np.expand_dims(np.linspace(-2 * np.pi, 2 * np.pi, num=200), axis=1)\n",
        "        np.random.RandomState(0).shuffle(x)\n",
        "        self.argsort_x = np.argsort(x.flatten())\n",
        "        y = np.sin(x)\n",
        "        super().__init__(x, y)\n",
        "\n",
        "        self.model = model\n",
        "        self.processed = 0\n",
        "\n",
        "        if use_graphics:\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            ax.set_xlim(-2 * np.pi, 2 * np.pi)\n",
        "            ax.set_ylim(-1.4, 1.4)\n",
        "            real, = ax.plot(x[self.argsort_x], y[self.argsort_x], color=\"blue\")\n",
        "            learned, = ax.plot([], [], color=\"red\")\n",
        "            text = ax.text(0.03, 0.97, \"\", transform=ax.transAxes, va=\"top\")\n",
        "            ax.legend([real, learned], [\"real\", \"learned\"])\n",
        "            plt.show(block=False)\n",
        "\n",
        "            self.fig = fig\n",
        "            self.learned = learned\n",
        "            self.text = text\n",
        "            self.last_update = time.time()\n",
        "\n",
        "    def iterate_once(self, batch_size):\n",
        "        for x, y in super().iterate_once(batch_size):\n",
        "            yield x, y\n",
        "            self.processed += batch_size\n",
        "\n",
        "            if use_graphics and time.time() - self.last_update > 0.1:\n",
        "                predicted = self.model.run(Constant(self.x)).data\n",
        "                loss = self.model.get_loss(\n",
        "                    Constant(self.x), Constant(self.y)).data\n",
        "                self.learned.set_data(self.x[self.argsort_x], predicted[self.argsort_x])\n",
        "                self.text.set_text(\"processed: {:,}\\nloss: {:.6f}\".format(\n",
        "                   self.processed, loss))\n",
        "                self.fig.canvas.draw_idle()\n",
        "                self.fig.canvas.start_event_loop(1e-3)\n",
        "                self.last_update = time.time()\n",
        "\n",
        "class DigitClassificationDataset(Dataset):\n",
        "    def __init__(self, model):\n",
        "        mnist_path = get_data_path(\"mnist.npz\")\n",
        "\n",
        "        with np.load(mnist_path) as data:\n",
        "            train_images = data[\"train_images\"]\n",
        "            train_labels = data[\"train_labels\"]\n",
        "            test_images = data[\"test_images\"]\n",
        "            test_labels = data[\"test_labels\"]\n",
        "            assert len(train_images) == len(train_labels) == 60000\n",
        "            assert len(test_images) == len(test_labels) == 10000\n",
        "            self.dev_images = test_images[0::2]\n",
        "            self.dev_labels = test_labels[0::2]\n",
        "            self.test_images = test_images[1::2]\n",
        "            self.test_labels = test_labels[1::2]\n",
        "\n",
        "        train_labels_one_hot = np.zeros((len(train_images), 10))\n",
        "        train_labels_one_hot[range(len(train_images)), train_labels] = 1\n",
        "\n",
        "        super().__init__(train_images, train_labels_one_hot)\n",
        "\n",
        "        self.model = model\n",
        "        self.epoch = 0\n",
        "\n",
        "        if use_graphics:\n",
        "            width = 20  # Width of each row expressed as a multiple of image width\n",
        "            samples = 100  # Number of images to display per label\n",
        "            fig = plt.figure()\n",
        "            ax = {}\n",
        "            images = collections.defaultdict(list)\n",
        "            texts = collections.defaultdict(list)\n",
        "            for i in reversed(range(10)):\n",
        "                ax[i] = plt.subplot2grid((30, 1), (3 * i, 0), 2, 1,\n",
        "                                         sharex=ax.get(9))\n",
        "                plt.setp(ax[i].get_xticklabels(), visible=i == 9)\n",
        "                ax[i].set_yticks([])\n",
        "                ax[i].text(-0.03, 0.5, i, transform=ax[i].transAxes,\n",
        "                           va=\"center\")\n",
        "                ax[i].set_xlim(0, 28 * width)\n",
        "                ax[i].set_ylim(0, 28)\n",
        "                for j in range(samples):\n",
        "                    images[i].append(ax[i].imshow(\n",
        "                        np.zeros((28, 28)), vmin=0, vmax=1, cmap=\"Greens\",\n",
        "                        alpha=0.3))\n",
        "                    texts[i].append(ax[i].text(\n",
        "                        0, 0, \"\", ha=\"center\", va=\"top\", fontsize=\"smaller\"))\n",
        "            ax[9].set_xticks(np.linspace(0, 28 * width, 11))\n",
        "            ax[9].set_xticklabels(\n",
        "                [\"{:.1f}\".format(num) for num in np.linspace(0, 1, 11)])\n",
        "            ax[9].tick_params(axis=\"x\", pad=16)\n",
        "            ax[9].set_xlabel(\"Probability of Correct Label\")\n",
        "            status = ax[0].text(\n",
        "                0.5, 1.5, \"\", transform=ax[0].transAxes, ha=\"center\",\n",
        "                va=\"bottom\")\n",
        "            plt.show(block=False)\n",
        "\n",
        "            self.width = width\n",
        "            self.samples = samples\n",
        "            self.fig = fig\n",
        "            self.images = images\n",
        "            self.texts = texts\n",
        "            self.status = status\n",
        "            self.last_update = time.time()\n",
        "\n",
        "    def iterate_once(self, batch_size):\n",
        "        self.epoch += 1\n",
        "\n",
        "        for i, (x, y) in enumerate(super().iterate_once(batch_size)):\n",
        "            yield x, y\n",
        "\n",
        "            if use_graphics and time.time() - self.last_update > 1:\n",
        "                dev_logits = self.model.run(Constant(self.dev_images)).data\n",
        "                dev_predicted = np.argmax(dev_logits, axis=1)\n",
        "                dev_probs = np.exp(SoftmaxLoss.log_softmax(dev_logits))\n",
        "                dev_accuracy = np.mean(dev_predicted == self.dev_labels)\n",
        "\n",
        "                self.status.set_text(\n",
        "                    \"epoch: {:d}, batch: {:d}/{:d}, validation accuracy: \"\n",
        "                    \"{:.2%}\".format(\n",
        "                        self.epoch, i, len(self.x) // batch_size, dev_accuracy))\n",
        "                for i in range(10):\n",
        "                    predicted = dev_predicted[self.dev_labels == i]\n",
        "                    probs = dev_probs[self.dev_labels == i][:, i]\n",
        "                    linspace = np.linspace(\n",
        "                        0, len(probs) - 1, self.samples).astype(int)\n",
        "                    indices = probs.argsort()[linspace]\n",
        "                    for j, (prob, image) in enumerate(zip(\n",
        "                            probs[indices],\n",
        "                            self.dev_images[self.dev_labels == i][indices])):\n",
        "                        self.images[i][j].set_data(image.reshape((28, 28)))\n",
        "                        left = prob * (self.width - 1) * 28\n",
        "                        if predicted[indices[j]] == i:\n",
        "                            self.images[i][j].set_cmap(\"Greens\")\n",
        "                            self.texts[i][j].set_text(\"\")\n",
        "                        else:\n",
        "                            self.images[i][j].set_cmap(\"Reds\")\n",
        "                            self.texts[i][j].set_text(predicted[indices[j]])\n",
        "                            self.texts[i][j].set_x(left + 14)\n",
        "                        self.images[i][j].set_extent([left, left + 28, 0, 28])\n",
        "                self.fig.canvas.draw_idle()\n",
        "                self.fig.canvas.start_event_loop(1e-3)\n",
        "                self.last_update = time.time()\n",
        "\n",
        "    def get_validation_accuracy(self):\n",
        "        dev_logits = self.model.run(Constant(self.dev_images)).data\n",
        "        dev_predicted = np.argmax(dev_logits, axis=1)\n",
        "        dev_accuracy = np.mean(dev_predicted == self.dev_labels)\n",
        "        return dev_accuracy\n",
        "\n",
        "class LanguageIDDataset(Dataset):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "        data_path = get_data_path(\"lang_id.npz\")\n",
        "\n",
        "        with np.load(data_path) as data:\n",
        "            self.chars = data['chars']\n",
        "            self.language_codes = data['language_codes']\n",
        "            self.language_names = data['language_names']\n",
        "\n",
        "            self.train_x = data['train_x']\n",
        "            self.train_y = data['train_y']\n",
        "            self.train_buckets = data['train_buckets']\n",
        "            self.dev_x = data['dev_x']\n",
        "            self.dev_y = data['dev_y']\n",
        "            self.dev_buckets = data['dev_buckets']\n",
        "            self.test_x = data['test_x']\n",
        "            self.test_y = data['test_y']\n",
        "            self.test_buckets = data['test_buckets']\n",
        "\n",
        "        self.epoch = 0\n",
        "        self.bucket_weights = self.train_buckets[:,1] - self.train_buckets[:,0]\n",
        "        self.bucket_weights = self.bucket_weights / float(self.bucket_weights.sum())\n",
        "\n",
        "        self.chars_print = self.chars\n",
        "        try:\n",
        "            print(u\"Alphabet: {}\".format(u\"\".join(self.chars)))\n",
        "        except UnicodeEncodeError:\n",
        "            self.chars_print = \"abcdefghijklmnopqrstuvwxyzaaeeeeiinoouuacelnszz\"\n",
        "            print(\"Alphabet: \" + self.chars_print)\n",
        "            self.chars_print = list(self.chars_print)\n",
        "            print(\"\"\"\n",
        "NOTE: Your terminal does not appear to support printing Unicode characters.\n",
        "For the purposes of printing to the terminal, some of the letters in the\n",
        "alphabet above have been substituted with ASCII symbols.\"\"\".strip())\n",
        "        print(\"\")\n",
        "\n",
        "        # Select some examples to spotlight in the monitoring phase (3 per language)\n",
        "        spotlight_idxs = []\n",
        "        for i in range(len(self.language_names)):\n",
        "            idxs_lang_i = np.nonzero(self.dev_y == i)[0]\n",
        "            idxs_lang_i = np.random.choice(idxs_lang_i, size=3, replace=False)\n",
        "            spotlight_idxs.extend(list(idxs_lang_i))\n",
        "        self.spotlight_idxs = np.array(spotlight_idxs, dtype=int)\n",
        "\n",
        "        # Templates for printing updates as training progresses\n",
        "        max_word_len = self.dev_x.shape[1]\n",
        "        max_lang_len = max([len(x) for x in self.language_names])\n",
        "\n",
        "        self.predicted_template = u\"Pred: {:<NUM}\".replace('NUM',\n",
        "            str(max_lang_len))\n",
        "\n",
        "        self.word_template = u\"  \"\n",
        "        self.word_template += u\"{:<NUM} \".replace('NUM', str(max_word_len))\n",
        "        self.word_template += u\"{:<NUM} ({:6.1%})\".replace('NUM', str(max_lang_len))\n",
        "        self.word_template += u\" {:<NUM} \".replace('NUM',\n",
        "            str(max_lang_len + len('Pred: ')))\n",
        "        for i in range(len(self.language_names)):\n",
        "            self.word_template += u\"|{}\".format(self.language_codes[i])\n",
        "            self.word_template += \"{probs[\" + str(i) + \"]:4.0%}\"\n",
        "\n",
        "        self.last_update = time.time()\n",
        "\n",
        "    def _encode(self, inp_x, inp_y):\n",
        "        xs = []\n",
        "        for i in range(inp_x.shape[1]):\n",
        "            if np.all(inp_x[:,i] == -1):\n",
        "                break\n",
        "            assert not np.any(inp_x[:,i] == -1), (\n",
        "                \"Please report this error in the project: batching by length was done incorrectly in the provided code\")\n",
        "            x = np.eye(len(self.chars))[inp_x[:,i]]\n",
        "            xs.append(Constant(x))\n",
        "        y = np.eye(len(self.language_names))[inp_y]\n",
        "        y = Constant(y)\n",
        "        return xs, y\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        exp = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp / np.sum(exp, axis=-1, keepdims=True)\n",
        "\n",
        "    def _predict(self, split='dev'):\n",
        "        if split == 'dev':\n",
        "            data_x = self.dev_x\n",
        "            data_y = self.dev_y\n",
        "            buckets = self.dev_buckets\n",
        "        else:\n",
        "            data_x = self.test_x\n",
        "            data_y = self.test_y\n",
        "            buckets = self.test_buckets\n",
        "\n",
        "        all_predicted = []\n",
        "        all_correct = []\n",
        "        for bucket_id in range(buckets.shape[0]):\n",
        "            start, end = buckets[bucket_id]\n",
        "            xs, y = self._encode(data_x[start:end], data_y[start:end])\n",
        "            predicted = self.model.run(xs)\n",
        "\n",
        "            all_predicted.extend(list(predicted.data))\n",
        "            all_correct.extend(list(data_y[start:end]))\n",
        "\n",
        "        all_predicted_probs = self._softmax(np.asarray(all_predicted))\n",
        "        all_predicted = np.asarray(all_predicted).argmax(axis=-1)\n",
        "        all_correct = np.asarray(all_correct)\n",
        "\n",
        "        return all_predicted_probs, all_predicted, all_correct\n",
        "\n",
        "    def iterate_once(self, batch_size):\n",
        "        assert isinstance(batch_size, int) and batch_size > 0, (\n",
        "            \"Batch size should be a positive integer, got {!r}\".format(\n",
        "                batch_size))\n",
        "        assert self.train_x.shape[0] >= batch_size, (\n",
        "            \"Dataset size {:d} is smaller than the batch size {:d}\".format(\n",
        "                self.train_x.shape[0], batch_size))\n",
        "\n",
        "        self.epoch += 1\n",
        "\n",
        "        for iteration in range(self.train_x.shape[0] // batch_size):\n",
        "            bucket_id = np.random.choice(self.bucket_weights.shape[0], p=self.bucket_weights)\n",
        "            example_ids = self.train_buckets[bucket_id, 0] + np.random.choice(\n",
        "                self.train_buckets[bucket_id, 1] - self.train_buckets[bucket_id, 0],\n",
        "                size=batch_size)\n",
        "\n",
        "            yield self._encode(self.train_x[example_ids], self.train_y[example_ids])\n",
        "\n",
        "            if use_graphics and time.time() - self.last_update > 0.5:\n",
        "                dev_predicted_probs, dev_predicted, dev_correct = self._predict()\n",
        "                dev_accuracy = np.mean(dev_predicted == dev_correct)\n",
        "\n",
        "                print(\"epoch {:,} iteration {:,} validation-accuracy {:.1%}\".format(\n",
        "                    self.epoch, iteration, dev_accuracy))\n",
        "\n",
        "                for idx in self.spotlight_idxs:\n",
        "                    correct = (dev_predicted[idx] == dev_correct[idx])\n",
        "                    word = u\"\".join([self.chars_print[ch] for ch in self.dev_x[idx] if ch != -1])\n",
        "\n",
        "                    print(self.word_template.format(\n",
        "                        word,\n",
        "                        self.language_names[dev_correct[idx]],\n",
        "                        dev_predicted_probs[idx, dev_correct[idx]],\n",
        "                        \"\" if correct else self.predicted_template.format(\n",
        "                            self.language_names[dev_predicted[idx]]),\n",
        "                        probs=dev_predicted_probs[idx,:],\n",
        "                    ))\n",
        "\n",
        "                self.last_update = time.time()\n",
        "\n",
        "    def get_validation_accuracy(self):\n",
        "        dev_predicted_probs, dev_predicted, dev_correct = self._predict()\n",
        "        dev_accuracy = np.mean(dev_predicted == dev_correct)\n",
        "        return dev_accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ty55qrMdonHH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test for correct execution with main() function"
      ],
      "metadata": {
        "id": "sVCYctmJvpVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # import models\n",
        "    # model = PerceptronModel(3)\n",
        "    # dataset = PerceptronDataset(model)\n",
        "    # model.train(dataset)\n",
        "\n",
        "    # model = RegressionModel()\n",
        "    # dataset = RegressionDataset(model)\n",
        "    # model.train(dataset)\n",
        "\n",
        "    model = DigitClassificationModel()\n",
        "    dataset = DigitClassificationDataset(model)\n",
        "    model.train(dataset)\n",
        "\n",
        "    # model = LanguageIDModel()\n",
        "    # dataset = LanguageIDDataset(model)\n",
        "    # model.train(dataset)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "vHQLdIzNvoLH",
        "outputId": "86efe560-0bd8-440e-e7a5-d5e314139568"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Epoch: 2\n",
            "Epoch: 3\n",
            "Epoch: 4\n",
            "Epoch: 5\n",
            "Epoch: 6\n",
            "Epoch: 7\n",
            "Epoch: 8\n",
            "Epoch: 9\n",
            "Epoch: 10\n",
            "Epoch: 11\n",
            "Epoch: 12\n",
            "Epoch: 13\n",
            "Epoch: 14\n",
            "Epoch: 15\n",
            "Epoch: 16\n",
            "Epoch: 17\n"
          ]
        }
      ]
    }
  ]
}